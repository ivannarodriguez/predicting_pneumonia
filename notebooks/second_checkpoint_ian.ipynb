{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Checkpoint\n",
    "### Predicting Pneumonia from X-Ray image\n",
    "\n",
    "Jimena Salinas Valdespino, Santiago Segovia Baquero, Stephania Tello Zamudio, Ivanna RodrÃ­guez Lobo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VW64heyUiHFn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # basic building block for neural neteorks\n",
    "import torch.nn.functional as F # import convolution functions like Relu\n",
    "import torch.optim as optim # optimzer\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pytorch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "IeESUPd2i0Qc",
    "outputId": "8d92bf29-f9be-4040-c44c-bb0001b564a6"
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir_path, resize=False, transform=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - csv_file (str): file path to the csv file\n",
    "            - img_dir_path: directory path to the images\n",
    "            - transform: Compose (a PyTorch Class) that strings together several\n",
    "              transform functions (e.g. data augmentation steps)\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(csv_file, skiprows=1, header=None)\n",
    "        self.img_dir = img_dir_path\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "        self.dimensions = self.get_dimensions()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns: (int) length of your dataset\n",
    "        \"\"\"\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and returns your sample (the image and the label) at the\n",
    "        specified index\n",
    "\n",
    "        Parameter: idx (int): index of interest\n",
    "\n",
    "        Returns: image, label\n",
    "        \"\"\"\n",
    "        img_path =  os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        \n",
    "        # read the image\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        # get the label\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        # apply transformations to image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # resize\n",
    "        if self.resize:\n",
    "            image = self.resize_image(image)\n",
    "        \n",
    "        if image.shape[0] > 1: # if it has more than one channels\n",
    "            g = T.Grayscale(1)\n",
    "            image = g(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def get_dimensions(self):\n",
    "        \"\"\"\n",
    "        This method creates a dictionary with the unique combinations of heightxwidth\n",
    "        for each image in the dataset.\n",
    "\n",
    "        returns a dictionary with dimensions as keys and the number of images\n",
    "            with that dimension as values\n",
    "        \"\"\"\n",
    "        dimensions = {}\n",
    "        for index in range(len(self.img_labels)):\n",
    "            image = self[index][0]\n",
    "            if self.resize:\n",
    "                image = self.resize_image(image)\n",
    "            _, height, width = image.shape\n",
    "            dimensions[(height,width)] = dimensions.get((height,width),0) + 1\n",
    "\n",
    "        return dimensions\n",
    "    \n",
    "    def resize_image(self,image):\n",
    "        \"\"\"\n",
    "        If the resize parameter==True, then all the images are\n",
    "        converted to a 150x150 size.\n",
    "\n",
    "        returns the resized image\n",
    "        \"\"\"\n",
    "        transform = T.Resize((150,150))\n",
    "        \n",
    "        return transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we implemented the Dataset class, we create one object per Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 16 unique height x width combinations in our validation data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are different sizes, and some very large images, we want to standarize the size of all images. We do this by passing a `resize` boolean parameter to our `CustomImageDataset` class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, we can call on the `dictionary` attribute for each one of our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(150, 150): 5216}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(150, 150): 16}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(150, 150): 624}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "In order to avoid overfitting, we need to do image augmentation for our training\n",
    "dataset. We do this below. We decided to augment the images following some \n",
    "examples of people who worked with this dataset in Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations we used are the following: rotate the image by 30 degrees, zoom into the image by 20%, flip the image horizontally, increase the image's sharpness, and change the color depth of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.RandomRotation(30),\n",
    "    T.RandomResizedCrop(size=(150, 150), scale=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomAdjustSharpness(sharpness_factor=2),\n",
    "    T.RandomPosterize(bits=4),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the above transformations to our training dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transforms to the training and validation dataset\n",
    "train_data = CustomImageDataset(csv_file = 'data_train.csv',\n",
    "                                img_dir_path = './',\n",
    "                                resize=True,\n",
    "                                transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = CustomImageDataset(csv_file = 'data_val.csv',\n",
    "                                img_dir_path = './',\n",
    "                                resize=True,\n",
    "                                transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomImageDataset(csv_file = 'data_test.csv',\n",
    "                                img_dir_path = './',\n",
    "                                resize=True,\n",
    "                                transform=train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our DataLoader\n",
    "\n",
    "Below, we create our DataLoader. The purpose of doing this is to load our data in\n",
    "batches to fit and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, \n",
    "                              batch_size=4, \n",
    "                              shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_data, \n",
    "                            batch_size=4, \n",
    "                            shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, \n",
    "                             batch_size=4, \n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "      \n",
    "      # inspire by Turing award winning LeCun, Bengio and Hinton's paper from 1998\n",
    "      # https://ieeexplore.ieee.org/document/726791 (cited more than 25,000 times!!!!!!!!!)\n",
    "      # code from https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/ \n",
    "      \n",
    "        # self.LeNet = nn.Sequential(     \n",
    "        # convolutional layers\n",
    "        \n",
    "        self.Layer1 = nn.Sequential(                                            # FIRST LAYER: (INPUT LAYER)\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),    # CONVOLUTION \n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))             # POOLING\n",
    "        self.Layer2 = nn.Sequential(                                            # SECOND LAYER: HIDDEN LAYER 1\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),   # CONVOLUTION \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))             # POOLING\n",
    "          # fully connected layers\n",
    "        self.F = nn.Flatten()\n",
    "        \n",
    "        self.LeNet = nn.Sequential(\n",
    "            nn.Sequential(                                            \n",
    "                nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),    \n",
    "                nn.BatchNorm2d(6),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size = 2, stride = 2)),\n",
    "            nn.Sequential(                                            \n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "      \n",
    "\n",
    "      # Calculate the input size for the linear layer\n",
    "        output_shape = self._get_conv_output_shape()\n",
    "        print(output_shape)\n",
    "        input_size = output_shape[0] * output_shape[1]\n",
    "\n",
    "      #self.classifier = nn.Sequential(\n",
    "        self.lin1 = nn.Linear(input_size, 64)\n",
    "        self.relu1 = nn.ReLU()       \n",
    "        self.lin2 = nn.Linear(64, 64)\n",
    "        self.relu2 = nn.ReLU()              \n",
    "        self.output = nn.Linear(64,2) \n",
    "                                                 \n",
    "    \n",
    "    def _get_conv_output_shape(self):\n",
    "      # Create a dummy tensor and pass it through the convolutional layers\n",
    "        x = torch.zeros((1, 1, 150, 150))\n",
    "        conv_output = self.LeNet(x)\n",
    "        return conv_output.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.Layer1(x)\n",
    "        print(\"dim after first conv layer:\", out.shape)\n",
    "        out = self.Layer2(out)\n",
    "        print(\"dim after second conv layer:\", out.shape)\n",
    "        out = self.F(out)\n",
    "        print(\"dim after flattening:\", out.shape)\n",
    "        out = self.lin1(out)\n",
    "        print(\"dim after first linear layer:\", out.shape)\n",
    "        out = self.relu1(out)\n",
    "        print(\"dim after first relu:\", out.shape)\n",
    "        out = self.lin2(out)\n",
    "        print(\"dim after second linear layer:\", out.shape)\n",
    "        out = self.relu2(out)\n",
    "        print(\"dim after first relu:\", out.shape)\n",
    "        out = self.output(out)\n",
    "        print(\"dim after output layer:\", out.shape)\n",
    "         \n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros((1, 1, 150, 150))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18496])\n"
     ]
    }
   ],
   "source": [
    "model = CustomNeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the prediction capacity of our model we compare the model's loss an classification power. The first one is defined as the accuracy of the model in terms of the predicted probabilities, while the second refers to how good the model classifies the actual labels (*e.g.*, normal lungs or lungs with pneumonia).\n",
    "\n",
    "The loss functions that we will use are:\n",
    "\n",
    "1. Binary cross-entropy:\n",
    "2. Cross-entropy:\n",
    "\n",
    "On the other hand, the accuracy measures we'll use are:\n",
    "\n",
    "1. Accuracy:\n",
    "2. Recall:\n",
    "3. Precision:\n",
    "4. F1:\n",
    "\n",
    "**Falta dar una pequenia explicacion de estas medidas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 150, 150])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, labels = batch\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim after first conv layer: torch.Size([1, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([1, 16, 34, 34])\n",
      "dim after flattening: torch.Size([1, 18496])\n",
      "dim after first linear layer: torch.Size([1, 64])\n",
      "dim after first relu: torch.Size([1, 64])\n",
      "dim after second linear layer: torch.Size([1, 64])\n",
      "dim after first relu: torch.Size([1, 64])\n",
      "dim after output layer: torch.Size([1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0306, 0.0798]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.float()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function and optimizer:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianjeong/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n",
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([4, 1, 150, 150])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "torch.Size([4, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v1/hpfy1s7j1p14y62b8dycr0d00000gn/T/ipykernel_55223/338327612.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1163\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1164\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and validate the network\n",
    "EPOCHS = 1\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for _ in range(EPOCHS):  # loop over the dataset multiple times\n",
    "    # TRAIN\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train()\n",
    "    running_loss_train = 0.0\n",
    "    accuracies_train = []\n",
    "    f1_scores_train = []\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.float()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            print(\"input shape:\", inputs.shape)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            print(outputs.shape)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # keep track of the loss\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "            # ALSO CALCULATE YOUR ACCURACY METRIC\n",
    "            predicted = predicted.detach().numpy()\n",
    "            labels = labels.detach().numpy()\n",
    "\n",
    "            accuracy = metrics.accuracy_score(labels, predicted)\n",
    "            accuracies_train.append(accuracy)\n",
    "\n",
    "            f1score = metrics.f1_score(labels, predicted)\n",
    "            f1_scores_train.append(f1score)\n",
    "\n",
    "    #AVERAGE TRAINING LOSS  \n",
    "    avg_train_loss = running_loss_train / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "    # CALCULATE AVERAGE ACCURACY METRIC\n",
    "    avg_train_acc = sum(accuracies_train)/len(accuracies_train)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "\n",
    "    #VALIDATE\n",
    "    # in the validation part, we don't want to keep track of the gradients \n",
    "    model.eval()\n",
    "    running_loss_val = 0.0\n",
    "    accuracies_val = []\n",
    "    f1_scores_val = []\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.float()\n",
    "\n",
    "        # val prediction\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        # keep track of the loss\n",
    "        running_loss_val += loss.item()\n",
    "\n",
    "        # ALSO CALCULATE YOUR ACCURACY METRIC\n",
    "        predicted = predicted.detach().numpy()\n",
    "        labels = labels.detach().numpy()\n",
    "\n",
    "        accuracy = metrics.accuracy_score(labels, predicted)\n",
    "        accuracies_val.append(accuracy)\n",
    "\n",
    "        f1score = metrics.f1_score(labels, predicted)\n",
    "        f1_scores_val.append(f1score)\n",
    "\n",
    "    # AVERAGE VALIDATION LOSS\n",
    "    avg_val_loss = running_loss_val / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "    # CALCULATE AVERAGE ACCURACY METRIC\n",
    "    avg_val_acc = sum(accuracies_val)/len(accuracies_val)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_acc)            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42117161921611646]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8024009146341463]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1948494911193848]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5625]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n",
      "dim after first conv layer: torch.Size([4, 6, 73, 73])\n",
      "dim after second conv layer: torch.Size([4, 16, 34, 34])\n",
      "dim after flattening: torch.Size([4, 18496])\n",
      "dim after first linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after second linear layer: torch.Size([4, 64])\n",
      "dim after first relu: torch.Size([4, 64])\n",
      "dim after output layer: torch.Size([4, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v1/hpfy1s7j1p14y62b8dycr0d00000gn/T/ipykernel_55223/2887109935.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracies_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mf1_scores_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v1/hpfy1s7j1p14y62b8dycr0d00000gn/T/ipykernel_55223/807477650.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# apply transformations to image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# resize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0mangle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, expand, center, fill, resample)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;31m# we need to set -angle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_inverse_affine_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36mrotate\u001b[0;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;31m# grid will be generated on the same device as theta and img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gen_affine_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_apply_grid_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py\u001b[0m in \u001b[0;36m_gen_affine_grid\u001b[0;34m(theta, w, h, ow, oh)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0mrescaled_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m     \u001b[0moutput_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaled_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "\n",
    "model.eval()\n",
    "running_loss_test = 0.0\n",
    "accuracies_test = []\n",
    "f1_scores_test = []\n",
    "for i, data in enumerate(test_dataloader):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.float()\n",
    "\n",
    "    # test prediction\n",
    "    outputs = model(inputs)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # ALSO CALCULATE YOUR ACCURACY METRIC\n",
    "    predicted = predicted.detach().numpy()\n",
    "    labels = labels.detach().numpy()\n",
    "\n",
    "    accuracy = metrics.accuracy_score(labels, predicted)\n",
    "    accuracies_test.append(accuracy)\n",
    "\n",
    "    f1score = metrics.f1_score(labels, predicted)\n",
    "    f1_scores_test.append(f1score)\n",
    "\n",
    "# AVERAGE VALIDATION LOSS\n",
    "avg_test_loss = running_loss_test / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "# CALCULATE AVERAGE ACCURACY METRIC\n",
    "avg_test_acc = sum(accuracies_test)/len(accuracies_test)   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
