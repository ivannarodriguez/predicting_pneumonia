{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters tuning\n",
    "### Predicting Pneumonia from X-Ray image\n",
    "\n",
    "Jimena Salinas Valdespino, Santiago Segovia Baquero, Stephania Tello Zamudio, Ivanna RodrÃ­guez Lobo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "VW64heyUiHFn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # basic building block for neural neteorks\n",
    "import torch.nn.functional as F # import convolution functions like Relu\n",
    "import torch.optim as optim # optimzer\n",
    "\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pytorch Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "IeESUPd2i0Qc",
    "outputId": "8d92bf29-f9be-4040-c44c-bb0001b564a6"
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir_path, resize=False, transform=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - csv_file (str): file path to the csv file\n",
    "            - img_dir_path: directory path to the images\n",
    "            - transform: Compose (a PyTorch Class) that strings together several\n",
    "              transform functions (e.g. data augmentation steps)\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(csv_file, skiprows=1, header=None)\n",
    "        self.img_dir = img_dir_path\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "        self.dimensions = self.get_dimensions()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns: (int) length of your dataset\n",
    "        \"\"\"\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Loads and returns your sample (the image and the label) at the\n",
    "        specified index\n",
    "\n",
    "        Parameter: idx (int): index of interest\n",
    "\n",
    "        Returns: image, label\n",
    "        \"\"\"\n",
    "        img_path =  os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        \n",
    "        # read the image\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        # get the label\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "\n",
    "        # apply transformations to image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # resize\n",
    "        if self.resize:\n",
    "            image = self.resize_image(image)\n",
    "        \n",
    "        if image.shape[0] > 1: # if it has more than one channels\n",
    "            g = T.Grayscale(1)\n",
    "            image = g(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def get_dimensions(self):\n",
    "        \"\"\"\n",
    "        This method creates a dictionary with the unique combinations of heightxwidth\n",
    "        for each image in the dataset.\n",
    "\n",
    "        returns a dictionary with dimensions as keys and the number of images\n",
    "            with that dimension as values\n",
    "        \"\"\"\n",
    "        dimensions = {}\n",
    "        for index in range(len(self.img_labels)):\n",
    "            image = self[index][0]\n",
    "            if self.resize:\n",
    "                image = self.resize_image(image)\n",
    "            _, height, width = image.shape\n",
    "            dimensions[(height,width)] = dimensions.get((height,width),0) + 1\n",
    "\n",
    "        return dimensions\n",
    "    \n",
    "    def resize_image(self,image):\n",
    "        \"\"\"\n",
    "        If the resize parameter==True, then all the images are\n",
    "        converted to a 150x150 size.\n",
    "\n",
    "        returns the resized image\n",
    "        \"\"\"\n",
    "        transform = T.Resize((150,150))\n",
    "        \n",
    "        return transform(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we implemented the Dataset class, we create one object per Dataset:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 16 unique height x width combinations in our validation data set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are different sizes, and some very large images, we want to standarize the size of all images. We do this by passing a `resize` boolean parameter to our `CustomImageDataset` class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Augmentation\n",
    "\n",
    "In order to avoid overfitting, we need to do image augmentation for our training\n",
    "dataset. We do this below. We decided to augment the images following some \n",
    "examples of people who worked with this dataset in Kaggle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations we used are the following: rotate the image by 30 degrees, zoom into the image by 20%, flip the image horizontally, increase the image's sharpness, and change the color depth of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.RandomRotation(30),\n",
    "    T.RandomResizedCrop(size=(150, 150), scale=(0.8, 1.2)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomAdjustSharpness(sharpness_factor=2),\n",
    "    T.RandomPosterize(bits=4),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply the above transformations to our datasets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transforms to the training and validation dataset\n",
    "train_data = CustomImageDataset(csv_file = '../data/data_train.csv',\n",
    "                                img_dir_path = '../data',\n",
    "                                resize=True,\n",
    "                                transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = CustomImageDataset(csv_file = '../data/data_val.csv',\n",
    "                              img_dir_path = '../data',\n",
    "                                resize=True,\n",
    "                                transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = CustomImageDataset(csv_file = '../data/data_test.csv',\n",
    "                               img_dir_path = '../data',\n",
    "                                resize=True,\n",
    "                                transform=train_transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our DataLoader\n",
    "\n",
    "Below, we create our DataLoader. The purpose of doing this is to load our data in\n",
    "batches to fit and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, \n",
    "                              batch_size=4, \n",
    "                              shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_data, \n",
    "                            batch_size=4, \n",
    "                            shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, \n",
    "                             batch_size=4, \n",
    "                             shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "      \n",
    "      # inspire by Turing award winning LeCun, Bengio and Hinton's paper from 1998\n",
    "      # https://ieeexplore.ieee.org/document/726791 (cited more than 25,000 times!!!!!!!!!)\n",
    "      # code from https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/ \n",
    "      \n",
    "        # self.LeNet = nn.Sequential(     \n",
    "        # convolutional layers\n",
    "        \n",
    "        self.Layer1 = nn.Sequential(                                            # FIRST LAYER: (INPUT LAYER)\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),    # CONVOLUTION \n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))             # POOLING\n",
    "        self.Layer2 = nn.Sequential(                                            # SECOND LAYER: HIDDEN LAYER 1\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),   # CONVOLUTION \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))             # POOLING\n",
    "          # fully connected layers\n",
    "        self.F = nn.Flatten()\n",
    "        \n",
    "        self.LeNet = nn.Sequential(\n",
    "            nn.Sequential(                                            \n",
    "                nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),    \n",
    "                nn.BatchNorm2d(6),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size = 2, stride = 2)),\n",
    "            nn.Sequential(                                            \n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0), \n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "      \n",
    "\n",
    "      # Calculate the input size for the linear layer\n",
    "        output_shape = self._get_conv_output_shape()\n",
    "        input_size = output_shape[0] * output_shape[1]\n",
    "\n",
    "      #self.classifier = nn.Sequential(\n",
    "        self.lin1 = nn.Linear(input_size, 64)\n",
    "        self.relu1 = nn.ReLU()       \n",
    "        self.lin2 = nn.Linear(64, 64)\n",
    "        self.relu2 = nn.ReLU()              \n",
    "        self.output = nn.Linear(64,2) \n",
    "                                                 \n",
    "    \n",
    "    def _get_conv_output_shape(self):\n",
    "      # Create a dummy tensor and pass it through the convolutional layers\n",
    "        x = torch.zeros((1, 1, 150, 150))\n",
    "        conv_output = self.LeNet(x)\n",
    "        return conv_output.shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.Layer1(x)\n",
    "        out = self.Layer2(out)\n",
    "        out = self.F(out)\n",
    "        out = self.lin1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.output(out)\n",
    "           \n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNeuralNetwork()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the prediction capacity of our model we compare the model's loss an classification power. The first one is defined as the accuracy of the model in terms of the predicted probabilities, while the second refers to how good the model classifies the actual labels (*e.g.*, normal lungs or lungs with pneumonia).\n",
    "\n",
    "The loss functions that we will use are:\n",
    "\n",
    "1. Binary cross-entropy\n",
    "2. Cross-entropy\n",
    "\n",
    "On the other hand, the accuracy measures we'll use are:\n",
    "\n",
    "1. Accuracy\n",
    "2. F1\n",
    "\n",
    "To assess the prediction capacity of our model we compare the model's loss an classification power. The first one is defined as the accuracy of the model in terms of the predicted probabilities, while the second refers to how good the model classifies the actual labels (*e.g.*, normal lungs or lungs with pneumonia).\n",
    "\n",
    "Given that we are interested in classifying only two classes, the loss function that we'll use is the **binary cross-entropy**, which is defined as:\n",
    "\n",
    "$$Loss = - \\frac{1}{N} \\sum_{i = 1}^{N} y_i \\cdot log(\\hat y_i) + (1 - y_i)\\cdot log(1 - \\hat y_i)$$\n",
    "\n",
    "where $y_i$ is the label for the $i$-th observation, and $\\hat y_i$ is the prediction. Intuitively the loss penalizes incorrect predictions more severely, with the logarithmic term amplifying the error when the predicted probability deviates from the true label. The overall loss is computed by averaging this binary cross-entropy loss over the entire training set.\n",
    "\n",
    "On the other hand, the accuracy measures we'll use are:\n",
    "\n",
    "1. **Accuracy**: provides an estimate of how well the model predicts the correct class labels\n",
    "\n",
    "2. **F1**: combines the precision and recall metrics into a single value, providing a balanced assessment of the model's performance. Is particularly important for our case given that we have an imbalance dataset that contains more pneumonia images vs. normal images.\n",
    "\n",
    "Both accuracy measures are going to be calculated for the train, validation and test dataset to evaluate the model's performance and potential overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 150, 150])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, labels = batch\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 146, 146]             156\n",
      "       BatchNorm2d-2          [-1, 6, 146, 146]              12\n",
      "              ReLU-3          [-1, 6, 146, 146]               0\n",
      "         MaxPool2d-4            [-1, 6, 73, 73]               0\n",
      "            Conv2d-5           [-1, 16, 69, 69]           2,416\n",
      "       BatchNorm2d-6           [-1, 16, 69, 69]              32\n",
      "              ReLU-7           [-1, 16, 69, 69]               0\n",
      "         MaxPool2d-8           [-1, 16, 34, 34]               0\n",
      "           Flatten-9                [-1, 18496]               0\n",
      "           Linear-10                   [-1, 64]       1,183,808\n",
      "             ReLU-11                   [-1, 64]               0\n",
      "           Linear-12                   [-1, 64]           4,160\n",
      "             ReLU-13                   [-1, 64]               0\n",
      "           Linear-14                    [-1, 2]             130\n",
      "================================================================\n",
      "Total params: 1,190,714\n",
      "Trainable params: 1,190,714\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.09\n",
      "Forward/backward pass size (MB): 5.20\n",
      "Params size (MB): 4.54\n",
      "Estimated Total Size (MB): 9.83\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, x.shape[2],x.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function and optimizer:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate the network\n",
    "EPOCHS = 10\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "train_f1 = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_f1 = []\n",
    "\n",
    "for e in range(EPOCHS):  # loop over the dataset multiple times\n",
    "    print('Analyzing epoch:', e + 1)\n",
    "    # TRAIN\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train()\n",
    "    running_loss_train = 0.0\n",
    "    accuracies_train = []\n",
    "    f1_scores_train = []\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs = inputs.float()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # keep track of the loss\n",
    "            running_loss_train += loss.item()\n",
    "            \n",
    "            # ALSO CALCULATE YOUR ACCURACY METRIC\n",
    "            predicted = predicted.detach().numpy()\n",
    "            labels = labels.detach().numpy()\n",
    "\n",
    "            accuracy = metrics.accuracy_score(labels, predicted)\n",
    "            accuracies_train.append(accuracy)\n",
    "\n",
    "            f1score = metrics.f1_score(labels, predicted)\n",
    "            f1_scores_train.append(f1score)\n",
    "\n",
    "    #AVERAGE TRAINING LOSS  \n",
    "    avg_train_loss = running_loss_train / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "    train_losses.append(avg_train_loss)\n",
    "    # CALCULATE AVERAGE ACCURACY METRIC\n",
    "    avg_train_acc = sum(accuracies_train)/len(accuracies_train)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "    #CALCULATE AVERAGE F1 SCORE\n",
    "    avg_train_f1 = sum(f1_scores_train)/len(f1_scores_train)\n",
    "    train_f1.append(avg_train_f1) \n",
    "\n",
    "    #VALIDATE\n",
    "    # in the validation part, we don't want to keep track of the gradients \n",
    "    model.eval()\n",
    "    running_loss_val = 0.0\n",
    "    accuracies_val = []\n",
    "    f1_scores_val = []\n",
    "    for i, data in enumerate(val_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.float()\n",
    "\n",
    "        # val prediction\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        # keep track of the loss\n",
    "        running_loss_val += loss.item()\n",
    "\n",
    "        # ALSO CALCULATE YOUR ACCURACY METRIC\n",
    "        predicted = predicted.detach().numpy()\n",
    "        labels = labels.detach().numpy()\n",
    "\n",
    "        accuracy = metrics.accuracy_score(labels, predicted)\n",
    "        accuracies_val.append(accuracy)\n",
    "\n",
    "        f1score = metrics.f1_score(labels, predicted)\n",
    "        f1_scores_val.append(f1score)\n",
    "\n",
    "    # AVERAGE VALIDATION LOSS\n",
    "    avg_val_loss = running_loss_val / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "    val_losses.append(avg_val_loss)\n",
    "    # CALCULATE AVERAGE ACCURACY METRIC\n",
    "    avg_val_acc = sum(accuracies_val)/len(accuracies_val)\n",
    "    val_accuracies.append(avg_val_acc)     \n",
    "    #CALCULATE AVERAGE F1 SCORE\n",
    "    avg_val_f1 = sum(f1_scores_val)/len(f1_scores_val)\n",
    "    val_f1.append(avg_val_f1) \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "test_f1 = []\n",
    "\n",
    "for e in range(EPOCHS):  # loop over the dataset multiple times\n",
    "    print('Analyzing epoch:', e + 1)\n",
    "    model.eval()\n",
    "    running_loss_test = 0.0\n",
    "    accuracies_test = []\n",
    "    f1_scores_test = []\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        # test prediction\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # ALSO CALCULATE YOUR ACCURACY METRIC\n",
    "        predicted = predicted.detach().numpy()\n",
    "        labels = labels.detach().numpy()\n",
    "\n",
    "        accuracy = metrics.accuracy_score(labels, predicted)\n",
    "        accuracies_test.append(accuracy)\n",
    "\n",
    "        f1score = metrics.f1_score(labels, predicted)\n",
    "        f1_scores_test.append(f1score)\n",
    "\n",
    "    # AVERAGE VALIDATION LOSS\n",
    "    avg_test_loss = running_loss_test / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
    "    test_losses.append(avg_test_loss)\n",
    "    # CALCULATE AVERAGE ACCURACY METRIC\n",
    "    avg_test_acc = sum(accuracies_test)/len(accuracies_test)   \n",
    "    test_accuracies.append(avg_test_acc)  \n",
    "    #CALCULATE AVERAGE F1 SCORE\n",
    "    avg_test_f1 = sum(f1_scores_test)/len(f1_scores_test)\n",
    "    test_f1.append(avg_test_f1) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training, testing and validation accuracies\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i + 1 for i in range(0, EPOCHS, 1)]\n",
    "ax.plot(epochs_array, train_accuracies, color=\"steelblue\", label='Train')\n",
    "ax.plot(epochs_array, val_accuracies, color=\"grey\", label='Validation')\n",
    "ax.plot(epochs_array, test_accuracies, color=\"green\", label='Test')\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"accuracy\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training, testing and validation F1 scores\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,6))\n",
    "epochs_array = [i + 1 for i in range(0, EPOCHS, 1)]\n",
    "ax.plot(epochs_array, train_f1, color=\"steelblue\", label='Train')\n",
    "ax.plot(epochs_array, val_f1, color=\"grey\", label='Validation')\n",
    "ax.plot(epochs_array, test_f1, color=\"green\", label='Test')\n",
    "ax.grid(alpha=0.25)\n",
    "ax.legend(loc=\"lower right\", fontsize=16)\n",
    "ax.set_xlabel(\"epochs\", fontsize=16)\n",
    "ax.set_ylabel(\"f1 score\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above, it seems like the trained convolutional neural network model performs well on the training data with a stable training accuracy of 0.95 after the first epoch. However, the testing accuracy remains around 0.8, which indicates that the model might be overfitting to the training data. This could be due to the unbalanced nature of the training dataset, with almost three times more images of pneumonia than normal. This suggests that the model might be biased towards predicting pneumonia more often.\n",
    "\n",
    "The F1 score for training is around 0.95, which indicates that the model has a good balance between precision and recall for the training data. However, the F1 score for testing is 0.85, which suggests that the model's performance on the testing data is not as good as on the training data. This might be due to the unbalanced training dataset, which makes it difficult for the model to generalize well to new, unseen data.\n",
    "\n",
    "In order to improve the model, we plan to run the model using a binary cross entropy loss function since it is specifically desgined for binary classification problems. We expect this could help the model's perfomance since this function penalizes more heavily for misclassifying instances from the minority class (i.e., normal in this case), which could help to address the imbalance in the training dataset. Additionally, we can try introducing a regularization term in order to prevent overfitting and improve the generalization ability of the model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
