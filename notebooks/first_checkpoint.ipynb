{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# First Checkpoint\n",
        "### Predicting Pneumonia from X-Ray image\n",
        "\n",
        "Jimena Salinas Valdespino, Santiago Segovia Baquero, Stephania Tello Zamudio, Ivanna Rodr√≠guez Lobo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VW64heyUiHFn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils import data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a Pytorch Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "IeESUPd2i0Qc",
        "outputId": "8d92bf29-f9be-4040-c44c-bb0001b564a6"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir_path, resize=False, transform=None):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            - csv_file (str): file path to the csv file\n",
        "            - img_dir_path: directory path to the images\n",
        "            - transform: Compose (a PyTorch Class) that strings together several\n",
        "              transform functions (e.g. data augmentation steps)\n",
        "        \"\"\"\n",
        "        self.img_labels = pd.read_csv(csv_file, skiprows=1, header=None)\n",
        "        self.img_dir = img_dir_path\n",
        "        self.transform = transform\n",
        "        self.resize = resize\n",
        "        self.dimensions = self.get_dimensions()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns: (int) length of your dataset\n",
        "        \"\"\"\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads and returns your sample (the image and the label) at the\n",
        "        specified index\n",
        "\n",
        "        Parameter: idx (int): index of interest\n",
        "\n",
        "        Returns: image, label\n",
        "        \"\"\"\n",
        "        img_path =  os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        \n",
        "        # read the image\n",
        "        image = read_image(img_path)\n",
        "\n",
        "        # get the label\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "\n",
        "        # apply transformations to image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "    \n",
        "    def get_dimensions(self):\n",
        "        \"\"\"\n",
        "        This method creates a dictionary with the unique combinations of heightxwidth\n",
        "        for each image in the dataset.\n",
        "\n",
        "        returns a dictionary with dimensions as keys and the number of images\n",
        "            with that dimension as values\n",
        "        \"\"\"\n",
        "        dimensions = {}\n",
        "        for index in range(len(self.img_labels)):\n",
        "            image = self[index][0]\n",
        "            if self.resize:\n",
        "                image = self.resize_image(image)\n",
        "            _, height, width = image.shape\n",
        "            dimensions[(height,width)] = dimensions.get((height,width),0) + 1\n",
        "\n",
        "        return dimensions\n",
        "    \n",
        "    def resize_image(self,image):\n",
        "        \"\"\"\n",
        "        If the resize parameter==True, then all the images are\n",
        "        converted to a 150x150 size.\n",
        "\n",
        "        returns the resized image\n",
        "        \"\"\"\n",
        "        transform = T.Resize((150,150))\n",
        "        \n",
        "        return transform(image)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we implemented the Dataset class, we create one object per Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = CustomImageDataset(csv_file = '../data/data_train.csv',\n",
        "                                img_dir_path = '../data')\n",
        "\n",
        "val_data = CustomImageDataset(csv_file = '../data/data_val.csv',\n",
        "                              img_dir_path = '../data')\n",
        "\n",
        "test_data = CustomImageDataset(csv_file = '../data/data_test.csv',\n",
        "                               img_dir_path = '../data')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The labels for each one of the datasets are the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    3875\n",
              "0    1341\n",
              "Name: 1, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train dataset\n",
        "train_data.img_labels.iloc[:,1].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    8\n",
              "1    8\n",
              "Name: 1, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Validation dataset\n",
        "val_data.img_labels.iloc[:,1].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1    390\n",
              "0    234\n",
              "Name: 1, dtype: int64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test dataset\n",
        "test_data.img_labels.iloc[:,1].value_counts()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inside our CustomImageDataset class we defined a method to compute a count of each height x width combination in our data set. The cells below display our results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4366"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.dimensions\n",
        "len(train_data.dimensions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 4,366 unique height x width combinations in our training data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "598"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.dimensions\n",
        "len(test_data.dimensions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 598 unique height x width combinations in our testing data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.dimensions\n",
        "len(val_data.dimensions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 16 unique height x width combinations in our validation data set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given that there are different sizes, and some very large images, we want to standarize the size of all images. We do this by passing a `resize` boolean parameter to our `CustomImageDataset` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_data = CustomImageDataset(csv_file = '../data/data_train.csv',\n",
        "                                img_dir_path = '../data',\n",
        "                                resize=True)\n",
        "\n",
        "val_data = CustomImageDataset(csv_file = '../data/data_val.csv',\n",
        "                              img_dir_path = '../data',\n",
        "                              resize=True)\n",
        "\n",
        "test_data = CustomImageDataset(csv_file = '../data/data_test.csv',\n",
        "                               img_dir_path = '../data',\n",
        "                               resize=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a check, we can call on the `dictionary` attribute for each one of our datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(150, 150): 5216}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(150, 150): 16}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_data.dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(150, 150): 624}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data.dimensions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Augmentation\n",
        "\n",
        "In order to avoid overfitting, we need to do image augmentation for our training\n",
        "dataset. We do this below. We decided to augment the images following some \n",
        "examples of people who worked with this dataset in Kaggle."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The transformations we used are the following: rotate the image by 30 degrees, zoom into the image by 20%, flip the image horizontally, increase the image's sharpness, and change the color depth of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = T.Compose([\n",
        "    T.RandomRotation(30),\n",
        "    T.RandomResizedCrop(size=(150, 150), scale=(0.8, 1.2)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomAdjustSharpness(sharpness_factor=2),\n",
        "    T.RandomPosterize(bits=4),\n",
        "    T.Grayscale(1)\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then apply the above transformations to our training dataset below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/stello/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Apply the transforms to the training dataset\n",
        "train_data = CustomImageDataset(csv_file = '../data/data_train.csv',\n",
        "                                img_dir_path = '../data',\n",
        "                                resize=True,\n",
        "                                transform=train_transform)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating our DataLoader\n",
        "\n",
        "Below, we create our DataLoader. The purpose of doing this is to load our data in\n",
        "batches to fit and test our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, \n",
        "                              batch_size=64, \n",
        "                              shuffle=True)\n",
        "\n",
        "val_dataloader = DataLoader(val_data, \n",
        "                            batch_size=64, \n",
        "                            shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, \n",
        "                             batch_size=64, \n",
        "                             shuffle=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
